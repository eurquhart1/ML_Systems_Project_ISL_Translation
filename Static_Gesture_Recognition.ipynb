{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a596cf-abb4-4dc2-9b7f-f63b748d7f94",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133c0e55-c2ba-4130-8072-6335283045e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6876718-a6bd-4a19-87d5-813bd85fae21",
   "metadata": {},
   "source": [
    "# 2. Read Images from the dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c92e0-d099-4a05-8141-c04005aea084",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"./Frames/\"\n",
    "exclude_letters=['X', 'J', 'Z']\n",
    "\n",
    "# Read the images from the 6 zip files and label them according to the letter they represent\n",
    "# Return two numpy arrays containing the images and their corresponding labels\n",
    "def load_and_label_images_from_zip():\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for zip_file in os.listdir(base_path):\n",
    "        if zip_file.endswith('.zip'):\n",
    "            zip_path = os.path.join(base_path, zip_file)\n",
    "\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                for image_file in zip_ref.namelist():\n",
    "                    if not any(excluded_letter in image_file for excluded_letter in exclude_letters):\n",
    "                        with zip_ref.open(image_file) as file:\n",
    "                            file_data = file.read()\n",
    "                            image_stream = BytesIO(file_data)\n",
    "                            image_array = np.frombuffer(image_stream.getvalue(), np.uint8)\n",
    "\n",
    "                            image = cv2.imdecode(image_array, cv2.IMREAD_UNCHANGED)\n",
    "                            flipped_image = cv2.flip(image, 1)\n",
    "                            label = image_file.split('-')[1]  # Extracting label from filename\n",
    "                            images.append(flipped_image)\n",
    "                            labels.append(label)\n",
    "\n",
    "    combined = list(zip(images, labels))\n",
    "\n",
    "    # Shuffle the combined list. This will introduce randomness into the training batches\n",
    "    random.shuffle(combined)\n",
    "\n",
    "    # Unzip the combined list back into images and labels\n",
    "    images, labels = zip(*combined)\n",
    "\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fe53c7-82cc-4d3b-ab96-761f9895fdf2",
   "metadata": {},
   "source": [
    "# 3. Apply Hand Landmarks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c07e916-e567-4919-9332-e812052e3574",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "def apply_landmarks():\n",
    "    images, labels = imgreader.load_and_label_images_from_zip()\n",
    "\n",
    "    with open('landmarks.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header\n",
    "        header = ['label']\n",
    "        for i in range(21):\n",
    "            header += [f'x{i}', f'y{i}', f'z{i}']\n",
    "        writer.writerow(header)\n",
    "\n",
    "        with mp_hands.Hands(\n",
    "            static_image_mode=True,\n",
    "            max_num_hands=2,\n",
    "            min_detection_confidence=0.5) as hands:\n",
    "\n",
    "            for image, label in zip(images, labels):\n",
    "                # Convert the grayscale image to RGB before processing.\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "                results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "                if results.multi_hand_landmarks:\n",
    "                    for hand_landmarks in results.multi_hand_landmarks:\n",
    "                        # Extract landmarks\n",
    "                        landmarks = [label]  # Start with the label\n",
    "                        for lm in hand_landmarks.landmark:\n",
    "                            landmarks.extend([lm.x, lm.y, lm.z])\n",
    "                        writer.writerow(landmarks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efef7a40-834d-4d82-aca0-0071fdc7e178",
   "metadata": {},
   "source": [
    "# 4. Data Preprocessing and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2da707-d00a-4a84-8a31-e89bfe00fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def augment_landmarks(landmarks, angles, scales):\n",
    "    augmented_data = []\n",
    "\n",
    "    for angle in angles:\n",
    "        for scale in scales:\n",
    "            r = R.from_euler('xyz', [angle, angle, angle], degrees=True)\n",
    "            rotated_landmarks = r.apply(landmarks)\n",
    "\n",
    "            # Apply scaling\n",
    "            scaled_landmarks = rotated_landmarks * scale\n",
    "            augmented_data.append(scaled_landmarks.flatten())\n",
    "\n",
    "    return augmented_data\n",
    "\n",
    "df = pd.read_csv('landmarks.csv')\n",
    "\n",
    "# Define angles and scales for augmentation\n",
    "angles = np.arange(0, 360, 36)\n",
    "scales = np.arange(0.8, 1.2, 0.1)\n",
    "\n",
    "augmented_data = []\n",
    "\n",
    "# Apply augmentation to each row\n",
    "for _, row in df.iterrows():\n",
    "    # Extract label and landmarks\n",
    "    label = row['label']\n",
    "    landmarks = np.array(row[1:]).reshape(-1, 3)\n",
    "\n",
    "    # Augment landmarks\n",
    "    augmented_landmarks = augment_landmarks(landmarks, angles, scales)\n",
    "\n",
    "    for augmented_set in augmented_landmarks:\n",
    "        augmented_row = np.hstack([label, augmented_set])\n",
    "        augmented_data.append(augmented_row)\n",
    "\n",
    "# Create a new DataFrame\n",
    "augmented_df = pd.DataFrame(augmented_data, columns=df.columns)\n",
    "\n",
    "# Save the augmented data\n",
    "augmented_df.to_csv('augmented_landmarks.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c720e7-dc12-4225-8788-e2e860d7f2c3",
   "metadata": {},
   "source": [
    "# 5. Train and Evaluate Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2878f032-a8c8-46ee-b110-e399759fd5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('./augmented_landmarks.csv')\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = df.drop('label', axis=1)\n",
    "y = df['label']\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Assuming y contains categorical string labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "# Cross-validation loop\n",
    "fold = 1\n",
    "for train_index, test_index in kf.split(X_scaled):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(y.nunique(), activation='softmax')  # Adjust the number of neurons to match the number of classes\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=30, validation_split=0.2, batch_size=16)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'Fold {fold} Test accuracy: {test_accuracy}')\n",
    "    \n",
    "    # Check if this model is the best so far\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        best_model = model\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "best_model.save('neuralnet_kfoldtest.h5')\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'scaler.save')\n",
    "joblib.dump(label_encoder, 'label_encoder.save')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985eae2a-e719-4581-b873-9c275a360065",
   "metadata": {},
   "source": [
    "# 6. Create Real Time Interpreter based on this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eced895-027f-423b-9a02-00eb31009dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabnanny import verbose\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import warnings\n",
    "from collections import deque\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "MAX_PREDICTIONS = 10\n",
    "CONFIDENCE_THRESHOLD = 0.6\n",
    "recent_predictions = deque(maxlen=MAX_PREDICTIONS)\n",
    "\n",
    "model = load_model('neuralnet_097.h5')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "scaler = joblib.load('scaler.save')\n",
    "label_encoder = joblib.load('label_encoder.save')\n",
    "\n",
    "def process_landmarks(landmarks):\n",
    "    landmarks_array = np.array([[landmark.x, landmark.y, landmark.z] for landmark in landmarks.landmark]).flatten()\n",
    "    landmarks_scaled = scaler.transform([landmarks_array])\n",
    "    return landmarks_scaled\n",
    "\n",
    "while cap.isOpened():\n",
    "    warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        print(\"Ignoring empty camera frame.\")\n",
    "        continue\n",
    "\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            processed_landmarks = process_landmarks(hand_landmarks)\n",
    "            \n",
    "            prediction = model.predict(processed_landmarks, verbose=0)\n",
    "            predicted_class = np.argmax(prediction, axis=1)\n",
    "            confidence = np.max(prediction)\n",
    "\n",
    "            if confidence > CONFIDENCE_THRESHOLD:\n",
    "                recent_predictions.append(predicted_class[0])\n",
    "\n",
    "            # Use highest vote for prediction over last few frames\n",
    "            if len(recent_predictions) >= MAX_PREDICTIONS:\n",
    "                most_common = np.bincount(np.array(recent_predictions)).argmax()\n",
    "                gesture_name = label_encoder.inverse_transform([most_common])[0]\n",
    "                cv2.putText(image, f'Gesture: {gesture_name}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('MediaPipe Hands', image)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
