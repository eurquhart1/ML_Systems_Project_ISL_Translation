{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow opencv-python mediapipe scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keypoints using MP Hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "gestures = ['j', 'x', 'z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_hand_landmarks(image, results):\n",
    "    for landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, landmarks, mp_hands.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    for landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                                  mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    # Initialize empty arrays for left and right hand landmarks\n",
    "    lh_keypoints = []\n",
    "    rh_keypoints = []\n",
    "\n",
    "    # Check for left hand landmarks\n",
    "    if results.multi_hand_landmarks:\n",
    "        for landmarks in results.multi_hand_landmarks:\n",
    "            if landmarks.hand_type == mp.solutions.hands.HandType.LEFT:\n",
    "                lh_keypoints.extend([landmark.x, landmark.y, landmark.z] for landmark in landmarks.landmark)\n",
    "            elif landmarks.hand_type == mp.solutions.hands.HandType.RIGHT:\n",
    "                rh_keypoints.extend([landmark.x, landmark.y, landmark.z] for landmark in landmarks.landmark)\n",
    "\n",
    "    # Flatten and concatenate the left and right hand landmarks\n",
    "    lh = np.array(lh_keypoints).flatten() if lh_keypoints else np.zeros(21 * 3)\n",
    "    rh = np.array(rh_keypoints).flatten() if rh_keypoints else np.zeros(21 * 3)\n",
    "\n",
    "    return np.concatenate([lh, rh])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gestures = ['j', 'x', 'z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "# Set the root directory where the video folders are located\n",
    "root_data_dir = 'Videos_dynamic'\n",
    "target_dir = 'My_Data'\n",
    "gestures = ['j', 'x', 'z']\n",
    "video_numbers = [1, 2, 3]\n",
    "sequence_length = 60  # Number of frames to capture\n",
    "\n",
    "# get the indices of the frames that should be captured from the video clip\n",
    "def get_frame_indices(total_frames, desired_frames):\n",
    "    if total_frames >= desired_frames:\n",
    "        return np.round(np.linspace(0, total_frames - 1, desired_frames)).astype(int)\n",
    "    else:\n",
    "        # Repeat some frames if there are not enough frames in the video\n",
    "        repeat_factor = np.ceil(desired_frames / total_frames)\n",
    "        indices = np.arange(total_frames)\n",
    "        return np.tile(indices, int(repeat_factor))[:desired_frames]\n",
    "\n",
    "# Set mediapipe model to Hands\n",
    "with mp.solutions.hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    \n",
    "    for person in range(1, 7):\n",
    "        person_dir = os.path.join(root_data_dir, f'Person{person}')\n",
    "\n",
    "        for action in gestures:\n",
    "            # read in the video files\n",
    "            for video_number in video_numbers:\n",
    "                video_file = os.path.join(person_dir, f'{action} ({video_number}).mp4')\n",
    "                \n",
    "                # Open the video file\n",
    "                cap = cv2.VideoCapture(video_file)\n",
    "                if not cap.isOpened():\n",
    "                    print(f\"Error opening video file: {video_file}\")\n",
    "                    continue\n",
    "\n",
    "                # The same number of frames should be captured from each video, regardless of length. Sample frames evenly if video is too long\n",
    "                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                frame_indices = get_frame_indices(total_frames, sequence_length)\n",
    "\n",
    "                # Define CSV file path\n",
    "                csv_dir = os.path.join(target_dir, action)\n",
    "                os.makedirs(csv_dir, exist_ok=True)\n",
    "                csv_path = os.path.join(csv_dir, f'Person{person}_{action}_{video_number}.csv')\n",
    "\n",
    "                # Write the landmark data for each frame into a new row of a CSV file\n",
    "                with open(csv_path, mode='w', newline='') as file:\n",
    "                    writer = csv.writer(file)\n",
    "\n",
    "                    for frame_idx in frame_indices:\n",
    "                        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "                        ret, frame = cap.read()\n",
    "                        if not ret:\n",
    "                            print(f\"Error reading frame at index {frame_idx}. Using the last successful frame.\")\n",
    "                            continue\n",
    "        \n",
    "                        # Make detections using Hands\n",
    "                        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                        results = hands.process(image)\n",
    "    \n",
    "                        # Draw landmarks\n",
    "                        if results.multi_hand_landmarks:\n",
    "                            for landmarks in results.multi_hand_landmarks:\n",
    "                                mp.solutions.drawing_utils.draw_landmarks(image, landmarks, mp.solutions.hands.HAND_CONNECTIONS)\n",
    "\n",
    "                        # Export keypoints to CSV\n",
    "                        if results.multi_hand_landmarks:\n",
    "                            keypoints = [landmark for landmarks in results.multi_hand_landmarks for landmark in landmarks.landmark]\n",
    "                            row = [f'{action}/'] + [val for lm in keypoints for val in (lm.x, lm.y, lm.z)]\n",
    "                        else:\n",
    "                            row = [f'{action}/'] + [0] * 63  # Assuming 21 landmarks * 3 coordinates each\n",
    "    \n",
    "                        writer.writerow(row)\n",
    "                        #print(f'Saved frame {frame_idx} to: {csv_path}')  # Debugging print\n",
    "    \n",
    "                        # graceful break if press 'q'\n",
    "                        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                            break\n",
    "                \n",
    "                # Check the length of the CSV file to verify it is correct (60)\n",
    "                with open(csv_path, 'r') as check_file:\n",
    "                    row_count = sum(1 for row in csv.reader(check_file))\n",
    "                    print(f'CSV file {csv_path} has {row_count} rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(gestures)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## DATA AUGMENTATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "\n",
    "def scale_data(data, scale_factor):\n",
    "    # Convert data to a NumPy float array and apply scaling\n",
    "    data_array = np.array(data, dtype=float)\n",
    "    return data_array * scale_factor\n",
    "\n",
    "def rotate_data(data, angle, axis='z'):\n",
    "    # Rotation matrices for different axes\n",
    "    if axis == 'x':\n",
    "        rotation_matrix = np.array([[1, 0, 0], \n",
    "                                    [0, np.cos(angle), -np.sin(angle)],\n",
    "                                    [0, np.sin(angle), np.cos(angle)]])\n",
    "    elif axis == 'y':\n",
    "        rotation_matrix = np.array([[np.cos(angle), 0, np.sin(angle)], \n",
    "                                    [0, 1, 0],\n",
    "                                    [-np.sin(angle), 0, np.cos(angle)]])\n",
    "    elif axis == 'z':\n",
    "        rotation_matrix = np.array([[np.cos(angle), -np.sin(angle), 0], \n",
    "                                    [np.sin(angle), np.cos(angle), 0],\n",
    "                                    [0, 0, 1]])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid rotation axis\")\n",
    "\n",
    "    # Rotate each point\n",
    "    data_rotated = np.dot(data, rotation_matrix)\n",
    "    return data_rotated\n",
    "\n",
    "def process_file(file_path, scale_factor, rotation_angle, rotation_axis):\n",
    "    # Load data (only the first 64 columns)\n",
    "    data = pd.read_csv(file_path, usecols=range(64))\n",
    "\n",
    "    # Separate the first column (string labels) and the rest of the data\n",
    "    labels = data.iloc[:, 0]\n",
    "    numeric_data = data.iloc[:, 1:]\n",
    "\n",
    "    # Apply scaling and rotation to each frame\n",
    "    data_scaled_rotated = []\n",
    "    for index, row in numeric_data.iterrows():\n",
    "        frame = row.values.reshape(21, 3)  # 21 landmarks, each with x, y, z\n",
    "        frame_scaled = scale_data(frame, scale_factor)\n",
    "        frame_rotated = rotate_data(frame_scaled, rotation_angle, rotation_axis)\n",
    "        data_scaled_rotated.append(frame_rotated.flatten())\n",
    "\n",
    "    # Convert the list back to DataFrame and add the labels column\n",
    "    augmented_data = pd.DataFrame(data_scaled_rotated, columns=numeric_data.columns)\n",
    "    augmented_data.insert(0, data.columns[0], labels)\n",
    "\n",
    "    return augmented_data\n",
    "\n",
    "# Define parameters for augmentation\n",
    "scale_factors = [0.9, 1.1]  # Different scaling factors - for each video, make a copy a bit smaller, and a copy a bit larger\n",
    "rotation_angles = [math.radians(10), math.radians(20)]  # Rotation angles in radians\n",
    "rotation_axes = ['x', 'y', 'z']  # Axes of rotation\n",
    "\n",
    "for gesture in gestures:\n",
    "    directory = DATA_PATH + gesture\n",
    "    \n",
    "    # Process each file with each combination of scaling and rotation\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith('Person') and filename.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            for scale_factor in scale_factors:\n",
    "                for rotation_angle in rotation_angles:\n",
    "                    for axis in rotation_axes:\n",
    "                        augmented_data = process_file(file_path, scale_factor, rotation_angle, axis)\n",
    "                        \n",
    "                        # Save the augmented data with a descriptive filename\n",
    "                        new_filename = f\"augmented_scale_{scale_factor}_rot_{math.degrees(rotation_angle)}_axis_{axis}_{filename}\"\n",
    "                        augmented_data.to_csv(os.path.join(directory, new_filename), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CODE FOR READING IN LANDMARK VALUES FROM CSV FILE\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "sequences, labels = [], []\n",
    "num_columns = 63  # Number of columns to read from each row\n",
    "target_dir = DATA_PATH\n",
    "\n",
    "scale_factors = [0.9, 1.1]  # Different scaling factors\n",
    "rotation_angles = [math.radians(10), math.radians(20)]  # Rotation angles in radians\n",
    "rotation_axes = ['x', 'y', 'z']  # Axes of rotation\n",
    "\n",
    "for person in range(1, 7):\n",
    "    # Original files\n",
    "    for action in gestures:\n",
    "        for video_number in range(1, 4):\n",
    "            csv_path = os.path.join(target_dir, action, f'Person{person}_{action}_{video_number}.csv')\n",
    "\n",
    "            if os.path.exists(csv_path):\n",
    "                window = []\n",
    "\n",
    "                with open(csv_path, 'r') as file:\n",
    "                    reader = csv.reader(file)\n",
    "                    for row in reader:\n",
    "                        # Read only the first 21 columns (excluding the label in the first column)\n",
    "                        landmarks = [float(coordinate) for coordinate in row[1:num_columns + 1]]\n",
    "                        window.append(landmarks)\n",
    "\n",
    "                sequences.append(window)\n",
    "                labels.append(label_map[action])\n",
    "\n",
    "    # Augmented files\n",
    "    for action in gestures:\n",
    "        for video_number in range(1, 4):\n",
    "            for scale_factor in scale_factors:\n",
    "                for rotation_angle in rotation_angles:\n",
    "                    for axis in rotation_axes:\n",
    "                        new_filename = f\"augmented_scale_{scale_factor}_rot_{math.degrees(rotation_angle)}_axis_{axis}_{filename}\"\n",
    "            \n",
    "                        csv_path = os.path.join(target_dir, action, new_filename)\n",
    "            \n",
    "                        if os.path.exists(csv_path):\n",
    "                            window = []\n",
    "            \n",
    "                            with open(csv_path, 'r') as file:\n",
    "                                reader = csv.reader(file)\n",
    "                                for row in reader:\n",
    "                                    landmarks = [float(coordinate) for coordinate in row[1:num_columns + 1]]\n",
    "                                    window.append(landmarks)\n",
    "            \n",
    "                            sequences.append(window)\n",
    "                            labels.append(label_map[action])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some testing statements for debugging purposes, to make sure the shapes of the collected data are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(60,63)))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(len(gestures), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using TensorFlow with Keras\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Final Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('lstm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Compression with Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code has been derived from the TensorFlow Keras documentation: https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "\n",
    "# Specify the pruning parameters\n",
    "pruning_params = {\n",
    "    'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.0,\n",
    "                                                 final_sparsity=0.5,\n",
    "                                                 begin_step=0,\n",
    "                                                 end_step=1000)\n",
    "}\n",
    "\n",
    "# Wrap the model with pruning\n",
    "model_for_pruning = sparsity.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# Compile the pruned model\n",
    "model_for_pruning.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add the UpdatePruningStep callback\n",
    "callbacks = [\n",
    "    sparsity.UpdatePruningStep()\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "# Replace 'X_train', 'y_train' with your training data\n",
    "model_for_pruning.fit(X_train, y_train, epochs=1000, callbacks=callbacks)\n",
    "\n",
    "test_loss, test_accuracy = model_for_pruning.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Accuracy of the pruned model: {test_accuracy*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, model_for_pruning_accuracy = model_for_pruning.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Accuracy of the pruned model: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pruned model\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import tempfile\n",
    "\n",
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "pruned_keras_file = 'pruned_model_098.h5'\n",
    "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
    "print('Saved pruned Keras model to:', pruned_keras_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pruned model to TFLite format and save it\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "\n",
    "# It's necessary to specify the supported ops due to the nature of the LSTM network\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, \n",
    "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "pruned_tflite_model = converter.convert()\n",
    "\n",
    "pruned_tflite_file = 'pruned_model_tflite_098.h5'\n",
    "\n",
    "with open(pruned_tflite_file, 'wb') as f:\n",
    "  f.write(pruned_tflite_model)\n",
    "\n",
    "print('Saved pruned TFLite model to:', pruned_tflite_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gzipped_model_size(file):\n",
    "  # Returns size of gzipped model, in bytes.\n",
    "  import os\n",
    "  import zipfile\n",
    "\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "\n",
    "  return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size('final_lstm_model.h5')))\n",
    "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
    "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Compression with Post-Pruning Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, \n",
    "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_and_pruned_tflite_model = converter.convert()\n",
    "\n",
    "quantized_and_pruned_tflite_file = 'quantized_and_pruned_tflite_model.tflite'\n",
    "\n",
    "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
    "  f.write(quantized_and_pruned_tflite_model)\n",
    "\n",
    "print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n",
    "\n",
    "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size('final_lstm_model.h5')))\n",
    "print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_model(interpreter):\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on ever y image in the \"test\" dataset.\n",
    "  prediction_digits = []\n",
    "  for i, test_image in enumerate(X_test):\n",
    "    if i % 1000 == 0:\n",
    "      print('Evaluated on {n} results so far.'.format(n=i))\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  print('\\n')\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  prediction_digits = np.array(prediction_digits)\n",
    "  y_test_indices = np.argmax(y_test, axis=1)\n",
    "  accuracy = (prediction_digits == y_test_indices).mean()\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)\n",
    "print('Pruned TF test accuracy:', model_for_pruning_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Compression with Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "student_model = Sequential()\n",
    "student_model.add(LSTM(32, return_sequences=True, activation='relu', input_shape=(60,63)))\n",
    "student_model.add(LSTM(32, return_sequences=False, activation='relu'))\n",
    "student_model.add(Dense(16, activation='relu'))\n",
    "student_model.add(Dense(len(gestures), activation='softmax'))\n",
    "\n",
    "# Compile the student model\n",
    "student_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Distillation parameters\n",
    "temperature = 10.0\n",
    "\n",
    "def distillation_loss(y_true, y_pred, teacher_logits):\n",
    "    student_probs = tf.nn.softmax(y_pred / temperature)\n",
    "    teacher_probs = tf.nn.softmax(teacher_logits / temperature)\n",
    "    \n",
    "    return tf.reduce_mean(tf.keras.losses.categorical_crossentropy(teacher_probs, student_probs))\n",
    "\n",
    "train_loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy_metric = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.batch(32)  # Set your batch size\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "for epoch in range(20):\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        # Get teacher model's logits\n",
    "        teacher_logits = model.predict(x_batch)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            student_logits = student_model(x_batch, training=True)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = distillation_loss(y_batch, student_logits, teacher_logits)\n",
    "        \n",
    "        gradients = tape.gradient(loss, student_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, student_model.trainable_variables))\n",
    "\n",
    "        train_loss_metric.update_state(loss)\n",
    "        train_accuracy_metric.update_state(y_batch, student_logits)\n",
    "\n",
    "    # Print the metrics at the end of each epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {train_loss_metric.result():.4f}, Accuracy: {train_accuracy_metric.result() * 100:.2f}%\")\n",
    "\n",
    "test_loss, test_accuracy = student_model.evaluate(X_test, y_test)\n",
    "\n",
    "student_model.save('student_model.h5')\n",
    "\n",
    "# Print the final accuracy\n",
    "print(f'Final Accuracy of the Student Model: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Test Sample Videos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for testing pre-recorded samples from the webcam\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "# Initialize MediaPipe hand pose model\n",
    "mp_hands = mp.solutions.hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Function to convert .mov files to .mp4 using FFmpeg\n",
    "def convert_mov_to_mp4(input_file, output_file):\n",
    "    # Check if the output file already exists, and if so, skip conversion\n",
    "    if os.path.exists(output_file):\n",
    "        print(f'{output_file} already exists. Skipping conversion.')\n",
    "        return\n",
    "    \n",
    "    ffmpeg_path = r'C:\\Program Files\\ffmpeg-6.1.1-essentials_build\\bin\\ffmpeg.exe'\n",
    "    command = [ffmpeg_path, '-i', input_file, '-q:v', '0', '-pix_fmt', 'yuv420p', output_file]\n",
    "    \n",
    "    try:\n",
    "        subprocess.check_call(command)\n",
    "        print(f'Successfully converted: {input_file} to {output_file}')\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f'Error converting: {input_file} to {output_file}')\n",
    "        print(e)\n",
    "    except Exception as e:\n",
    "        print(f'An error occurred: {e}')\n",
    "\n",
    "# Path to the test video file\n",
    "convert_mov_to_mp4('test_videos/j7.mov', 'test_videos/j7.mp4')\n",
    "test_video_path = 'test_videos/j7.mp4'\n",
    "\n",
    "# Function to process a video frame and return hand landmarks\n",
    "def process_frame(frame, hands_model):\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands_model.process(image)\n",
    "    if results.multi_hand_landmarks:\n",
    "        keypoints = [landmark for landmarks in results.multi_hand_landmarks for landmark in landmarks.landmark]\n",
    "        landmarks = [val for lm in keypoints for val in (lm.x, lm.y, lm.z)]\n",
    "        # Ensure landmarks length is 63\n",
    "        if len(landmarks) == 63:\n",
    "            return landmarks \n",
    "        else:\n",
    "            print(\"zero\")\n",
    "            return [0] * 63\n",
    "    else:\n",
    "        return [0] * 63\n",
    "\n",
    "\n",
    "# Open the test video file\n",
    "cap = cv2.VideoCapture(test_video_path)\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error opening video file: {test_video_path}\")\n",
    "\n",
    "sequence_length = 60  # Same as your training data\n",
    "landmarks_list = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or len(landmarks_list) >= sequence_length:\n",
    "        break\n",
    "\n",
    "    landmarks = process_frame(frame, mp_hands)\n",
    "    landmarks_list.append(landmarks)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Ensure the sequence is of the correct length\n",
    "if len(landmarks_list) < sequence_length:\n",
    "    # Repeat some frames if there are not enough frames in the video\n",
    "    repeat_factor = np.ceil(sequence_length / len(landmarks_list))\n",
    "    landmarks_list = (landmarks_list * int(repeat_factor))[:sequence_length]\n",
    "\n",
    "# Format the collected landmarks for model prediction\n",
    "test_data = np.array(landmarks_list)\n",
    "test_data = test_data.reshape(1, sequence_length, 63)  # Reshape as needed for your model\n",
    "\n",
    "prediction = model.predict(test_data)\n",
    "\n",
    "predicted_index = np.argmax(prediction)\n",
    "\n",
    "predicted_gesture = gestures[predicted_index]\n",
    "\n",
    "print(\"Predicted Gesture:\", predicted_gesture)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
